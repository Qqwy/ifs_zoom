#+TITLE: \Huge In by Out again
#+SUBTITLE: Faking arbitrarily-deep zooming on Iterated Function Systems

#+BIND: org-latex-prefer-user-labels t

#+LATEX_HEADER: \setlength{\parindent}{1em}
#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[citestyle=alphabetic,bibstyle=alphabetic, hyperref=true, backref=true,maxcitenames=3,url=true,backend=biber,natbib=true] {biblatex}
#+LATEX_HEADER: \addbibresource{bibliography.bib}

#+LATEX_HEADER: \usepackage[a4paper, total={7in, 9in}]{geometry}

#+LATEX_HEADER: \usepackage[ruled, procnumbered]{algorithm2e}
#+LATEX_HEADER: \usepackage{listings}

# not emph
#+LATEX_HEADER: \SetArgSty{}

#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{pifont}
#+LATEX_HEADER: \newcommand{\cmark}{\color{ForestGreen}\ding{52}}%
#+LATEX_HEADER: \newcommand{\xmark}{\color{Maroon}\ding{55}}%

#+LATEX_HEADER: \hypersetup{colorlinks=true}

#+LATEX_HEADER: \renewcommand{\sectionautorefname}{{\color{Black}\S}}
#+LATEX_HEADER: \renewcommand{\subsectionautorefname}{{\color{Black}\S\S}}
#+LATEX_HEADER: \renewcommand{\subsubsectionautorefname}{{\color{Black}\S\S\S}}
#+LATEX_HEADER: \renewcommand{\functionautorefname}{{\color{Black}\textbf{Function}}\color{Magenta}}
#+LATEX_HEADER: \renewcommand{\algorithmautorefname}{{\color{Black}\textbf{Algorithm}}\color{Magenta}}


#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[shortlabels]{enumitem}

#+LATEX_HEADER: \usepackage{newfloat}
#+LATEX_HEADER: \DeclareFloatingEnvironment[fileext=lol, listname={List of L-system definitions}, name=L-system, placement=tbhp, within=section]{lsystem}

#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{todonotes}

#+LATEX_HEADER: \usepackage{pifont,kantlipsum}
#+LATEX_HEADER: \newcommand*{\altasterism}{\vspace*{1em plus .5em minus .5em}\noindent\hspace*{\fill}\ding{104}\hspace*{\fill}}



#+LATEX_HEADER: \usepackage{rugscriptie}
#+LATEX_HEADER: \supervisor{dr. J. Kosinka}
#+LATEX_HEADER: \supervisor{G. J. Hettinga}
#+LATEX_HEADER: \date{August 2020}
#+LATEX_HEADER: \faculty{fwn} % Or feb, fgg, fgmw, fl, frg, frw, fw, umcg
#+LATEX_HEADER: \thesistype{Bachelors's thesis} % Will be printed unmodified

#+OPTIONS: toc:4

\listoftodos


* Abstract

Iterated Function Systems are a mathematical approach to rendering fractals that sees wide usage in the modeling of physical phenomena, 
image compression, and the creation of abstract art.
Current IFS rendering techniques meant that up to now, interactively exploring an IFS required fully re-approximating the IFS' attractor at every frame.

This thesis proposes a combination of two techniques to enable the faster exploring of IFSs.
First, a point cloud is used as intermediate attractor approximation, that can be re-used between animation frames.
Secondly, a technique coined 'self-similarity jumping' is employed to keep the attractor representation detailed, even when zooming in very far.

A proof-of-concept computer program was implemented which shows that the usefullness of the employed techniques, while promising, 
is hampered by the fact that self-similarity jumping cannot be used in a couple of common situations.

* Introduction

Iterated Function Systems (IFSs) are a method to generate infinitely detailed fractal images 
by repeatedly applying simple mathematical functions until a fixed point is reached. [CITE]
IFSs see use in rendering/modeling of physical phenomena[CITE], image compression [CITE] and representing gene structures [CITE].
Sometimes they also see use simply for the aesthetic beauty of their graphical representations[CITE].

Various computer algorithms to visualize IFSs exist [CITE], [CITE].
However, these all take either a still image as final result, or, if they want to render an animation,
view this as a sequence of separate still images to generate.

This leaves a door open for potential optimization: if there is information that remains the same between animation frames, 
then we could compute it only once and re-use this information for all frames.

For instance, many kinds of animations consist of transformations of the camera viewport w.r.t the viewed fractal over time like translation, rotation and scaling do not require alterations to the fractal itself.
This means that (an approximation of) the fractal might be computed once and then be used for all frames.

Furthermore, because of the self-similar nature of the rendered fractals,
it might it be possible to simulate zooming in to an arbitrary depth by 'jumping up' to a more shallow camera viewport
that shares the same self-similarity as the original one.

In this thesis, these two ideas are investigated in detail.

** Overview

In the next section, autoref:section:background, Iterated Function Systems will be introduced and pre-existing methods to render their attractors in single-threaded and heavily parallel environments described.
This then leads to a clear definition of the research question in autoref:section:research_question.
The approach taken to test this question is described in autoref:section:approach, followed by qualitative results in autoref:section:findings.
We conclude in autoref:section:conclusion and finally hint at some approaches for further work in autoref:section:further_work


* Background
\label{section:background}

This section will describe the different building blocks necessary to formalate the research question.
First, IFSs are formalized, followed by a description of the different ways in which an IFS' attractor can be rendered, 
and how these techniques might be paralellized.

\pagebreak

** Informal description of an Iterated Function System
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/sierpinsky_iterations}
\caption{The first six iterations of the Sierpi\'nsky triangle IFS (\autoref{ifs:sierpinsky}). 
The initial image is just the unit square. We then iteratively combine the results of transforming the current image using one of the three transformations. 
The letters indicate which (sequence of) transformation(s) resulted in this part of the image.
Dashed red lines are used for the first four iterations to indicate the self-similarity between the previous iteration and the current one extra clearly.
Even after a couple of iterations it can be seen that the shape of the original image does not matter for the shape of the attractor.}
\label{figure:sierpinsky_iterations}
\end{figure}


Informally, an Iterated Function System is a set of transformations that, given any input image, can create a new image by

1. transforming the input image with each of the transformations
2. combining all transformed images together. This is the new image.

This process is then repeated an arbitrary number of times, until changes between the input image and output image are no longer visible to the human eye.

What you end up with is a visual representation of the IFS's attractor.
A simple example of this process can be seen in autoref:figure:sierpinsky_iterations

\todo[inline]{Some examples of IFSs? (that are used later on in the thesis)}


** Formal definition of an Iterated Function System

Formally, an Iterated Function System consists of a finite set of contraction mappings that map a complete metric space $(\mathcal{M}, d)$ to itself:

$$ = \{ f_i : \mathcal{M} \rightarrow \mathcal{M} | i = 1, 2, \ldots, N \}, N \in \mathbb{N}$$

That each mapping needs to be contractive means that for each mapping $f_i$, the distance between every two arbitrary points $a$ and $b$ in $(\mathcal{M}, d)$ needs to be larger than the distance of the points after transforming them:

$$d(f_i(a), f_i(b)) < d(a, b)$$

We can then take the union of performing all of these mappings on any compact set of points $\mathcal{S}_0 \subset \mathcal{M}$. This procedure is called the /Hutchkinson Operator/ ($H$). 
We can iterate it as often as we'd like:

$$ \mathcal{S}_{n + 1} = H(\mathcal{S}_n) = \bigcup_{i=1}^{N} f_i(\mathcal{S}_n) $$

If we perform this operation an arbitrary number of times, we approach the fixed-point or attractor, $\mathcal{A}$, of the Iterated Function System:

$$\mathcal{A} = \lim_{n \rightarrow \infty} \mathcal{S}_n$$

Curiously, which set of points $\mathcal{S}_0$ we started with makes no difference (we might even start with a single point) [CITE].

*** Restriction to affine transformations on the two-dimensional euclidean plane 

Most research of IFSs restricts itself to using $\mathbb{R}^2$ as metric space[fn:euclidean] which can easily be rendered to screen or paper,
and furthermore most commonly-used IFSs are restricted to use /affine transformations/ as mappings.

Because of their prevalence, these are also the restrictions that will be used in this thesis.

*** The viewport transformation
\label{subsection:viewport_transformation}

For any IFS with mappings we can transform its attractor by any invertible function $t$ by adjusting each of the mappings according to the
transform theorem [CITE fractals everywhere] $f_i' = t \cdot f_i \cdot t^{-1}$, essentially transforming points from the new space to the old space, then applying the mapping, and finally transforming them back to the new space. 
This allows us to give users freedom to choose any desired mappings and 'initial camera viewport' of the attractor,
while still allowing all calculations to happen with regard to the unit square (which keeps them simpler).

[fn:euclidean] More formally, the two-dimensional Euclidean space: $\left(\mathbb{R}^2, d(p, q) = \sqrt{p - q)^2}\right)$.

** Rendering an Iterated Function System

A couple of algoritms exist to render (visualize) the attarctor of an Iterated Function System. 
While it is impossible to render the attractor exactly, as this would require an infinite number of transformation steps,
we can approximate it until we are certain that the difference between our approximation and the attractor is smaller than
the smallest thing we can visually represent (e.g. smaller than the size of a pixel).

Because we apply $H$ many times and each time consists of taking the union of $N$ different transformations,
the result can be seen as traversing an (infinitely deep) tree of transformations, 
where each sub-tree is self-similar to the tree as a whole.

Different algorithms take different approaches to evaluating this tree (up to a chosen finite depth).

More in-depth information about the rendering of Iterated Function Systems can be found [CITE]. 
Short summaries of the two most common techniques will now follow.

# All of the rendering techniques have in common that the iteration of the Hutchkinson operator is seen as an (infinitely deep) tree with branching factor $N$.
# We traverse this tree up to a certain depth.

\todo[inline]{PICTURE OF THIS TREE}


*** The deterministic method

In this approach we evaluate the whole tree up to a chosen depth. The algorithm works as follows:

1. Pick a starting point $z_0$;
2. Traverse the tree down to the chosen depth $k$, building up a sequence of transformations [fn:function_composition]
   $f_{i_k} \circ \ldots \circ f_{i_1}$;
3. For each node at this depth, evaluate and render $z_k = (f_{i_k} \circ \ldots \circ f_{i_1})(z_0) = f_{i_k-1}(z_{k-1})$;

Since $z_{k} = f_{i_k-1}(z_{k-1})$ this procedure takes, for an approximation that consists of $N$ points, depending on the tree traversal chosen:

- a linear amount ( $\mathcal{O}(N)$ ) of memory  for a breadth-first tree-traversal.
- a logarithmic amount ( $\mathcal{O}(\log{N})$ ) of memory for a depth-first tree-traversal.

The advantage of the breadth-first traversal is that generation could be stopped interactively,
while the depth-first traversal requires the stopping criterion to be known beforehand. [CITE]

While the deterministic method is easy to understand (and indeed is a direct translation of the informal process described at the start of autoref:section:background),
it is usually less efficient and more complex to implement on a computer than the algorithm that will be described next.

[fn:function_composition] $\circ$ stands for function composition: $(f \circ g)(x) = f(g(x))$. 
Be aware that when affine transformation functions are represented as matrices (e.g. $F$ and $G$), matrix multiplication is in the opposite order ($f \circ g \equiv G \cdot F$)

*** The chaos game
\label{subsection:chaos_game}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_1000000}
         \caption{1,000,000 points}
         \label{figure:barnsley_mil}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_100000000}
         \caption{10,000,000}
         \label{figure:barnsley_ten_mil}
     \end{subfigure}
        \caption{The Barnsley Fern, rendered using the chaos game with different amounts of points.}
        \label{figure:barnsley_chaos_game_points}
\end{figure}


The /stochastic method/[CITE], also known as the /random iteration algorithm/[CITE] or more frequently the /chaos game/, works as seen in autoref:chaosGame

\begin{algorithm}[H]
\caption{the chaos game}
\label{chaosGame}
  $n$: the number of transformations the IFS consists of. \\
  $z$: a random point on the screen  \\
  \While{less than $N$ points plotted}{ 
    $i$: a random integer between $0$ and $n$.  \\
    $z = f_i(z)$  \\
    render($z$) except during the first $x$ iterations \\
  }

\end{algorithm}

This method converges to a correct result because of the following two facts:

- because the precision of the canvas we render on is finite, and because all transformations are contracting,
 two points $a$ and $b$ are indistinguishable after only $x$ transformations.
  In other words, only the latest $x$ transformations determine at what location on the canvas a point will end up (with the latest transformation having the largest effect on the point's final location).[fn:contraction]
- at each depth in the tree the subtree remains the same, so every sequence of transformations approaches the attractor.

Therefore, all intermediate points after the first $x$ iterations are visually indistinguishable from the a point that is part of the attractor.
By running this non-deterministic approach for enough iterations we approach a diverse enough set of 'transformation sequences of length $x$' that we end up covering the whole attractor.

The nice thing about the chaos game is that it does not require any extra memory (besides the point $z$).
Also, because it is so simple and little auxiliary memory is needed, it runs very efficiently on modern CPU architectures.

A disadvantage of the chaos game is that the result is by its very nature /non-deterministic/.
If not enough points are used, the result might end up 'grainy' and it is not predictable what part of the attractor will be covered (see autoref:figure:barnsley_chaos_game_points).


One further disadvantage the chaos game has, is that in its simplest form, all transformations have an equally likely chance to be used.
However, because some transformations might be (much) more contracting than others, this means that coverage of the attractor is not even,
which means that we need to use much more iterations than would be the case if we balance it out.

Therefore, most implementations of the chaos game allow (or require) the user to specify a /probability/ for each transformation.
All these probabilities together ought to sum up to 1.[fn:probabilities]

\altasterism

Because of its simplicity and computational efficiency, the chaos game is used more frequently than the deterministic method for practical implementations.
The chaos game is also easier to paralellize for Graphical Processor Units (GPUs), as will be outlined in the next subsection.

[fn:contraction] Methods for precisely determining the lower and upper bounds of IFS contraction for a particular IFS (and therefore the exact value of $x$) exist [CITE], 
but are not relevant for this thesis.

[fn:probabilities] These probabilities are often fine-tuned by hand, although algorithms to determine balanced probabilities exist as well [CITE] section 2.4.

** Paralellizing IFS rendering by using a Graphical Processor Unit

It is enticing to port IFS rendering to run on GPU-architecture because to produce a smooth image, often hundreds of millions of points are needed.

However, optimizing IFS rendering to run well on GPU-architectures is a bit of a challenge.

GPU shaders usually operate by running a check for every pixel on the final canvas, to determine its color.
For other fractals like the Mandelbrot- and Julia-sets, this is a natural fit since the construction of those fractals works exactly in that way.

For an IFS this does not work, as an IFS is created in the other direction. Points end up at some location on the canvas /only after transforming/ many times.
Attempts to go the other way fall flat, for instance because this would require to invert the IFS' mappings, but they are not guaranteed to be invertible.

Instead, General-Purpose GPU-techniques are used that are able to use the top-down approach.

*** The chaos game on the GPU
\label{subsection:chaos_game_gpu}

The deterministic method is difficult to paralellize on the GPU because of the extra memory that is required to keep track of the current position in the tree.
Coordinating which GPU thread would calculate which part of the tree and sharing results would be a hassle.

Instead, the chaos game is more frequently used because of its simplicity. It is paralellized in a straightforward way, by running the iteration process many times side-by-side (one per GPU thread),
and then combine the final results of all of these on a single canvas. [CITE]

*** The deterministic method on the GPU
\label{subsection:deterministic_gpu}

An exciting approach taken in [CITE] uses the deterministic method instead:
by using the fast inverse square root operation, even unbounded (noncontracting) and nonlinear IFSs can be efficiently
evaluated using the deterministic method, programmed in normal GPU shaders that manipulate a couple of GPU textures.


* Research Question
\label{section:research_question}

In the last section, the construction of an IFS's attractor was formally defined, 
and different approaches of rendering it were outlined.

While many different approaches to IFS rendering exist, some of them quite efficient,
none of them re-uses information from rendering one image of the IFS for rendering another.

This leads us to the research question of this thesis:

**Is it possible, by re-using information between animation frames, to render animations of an Iterated Function System's attractor in which the camera zooms in, in real-time?**

\todo[inline]{Maybe rephrase question?}

* Approach
\label{section:approach}

To put this to the test, a simple software program was created which calculates the IFS' attractor only once,
and then allows a user to interactively zoom and pan the camera around to investigate different parts of the attractor.

** Design

The inspiration of the design is two-fold:

First, we use the insight that the (parallel) chaos game can be used to generate a /point cloud/, allowing us to re-use parts of the computation between animation frames
and thus render each frames faster.

Second, while a point cloud only allows /zooming in/ up to a particular depth before losing considerable detail, 
it is possible to detect when we are looking at a self-similar part of the attractor.
This allows us, in many situations, to replace the current camera viewport with a more /shallow/ one, keeping the amount of detail high.

*** Point clouds

The main inspiration for the re-usability approach is that we can modify the GPU-variant of the chaos game algorithm outlined in autoref:subsection:chaos_game_gpu
to render to a /point cloud/ instead of immediately to a canvas.
When we then move around the camera, we are able to re-use the points in the point cloud;
only where the points in the point cloud end up on screen exactly needs to be re-calculated, 
by transforming all of the points exactly once with the 'view transformation' 
(and culling all points outside of the viewport).

This is faster than re-evaluating the whole attractor using the chaos game at every frame which would require transforming all points /many/ times.

Formally, to render an attractor approximation consisting of $N$ points, 
running the whole chaos game each frame takes $2(N+x)$ transformations per frame. [fn:definitions] 
Running this on $p$ parallel threads has a time complexity of $\mathcal{O}(\frac{2(N+px)}{p})$.

Unoptimized, it takes $N$ transformations to render a precomputed point cloud to screen each frame (in parallel this corresponds to a time complexity of $\mathcal{O}(\frac{N}{p})$).
This does not seem very impressive since $\mathcal{O}(\frac{2(N+x)}{p}) \approx \mathcal{O}(\frac{2N}{p}) \approx \mathcal{O}(\frac{N}{p})$,
placing the two approaches in the same order of efficiency. However, it is possible to optimize point cloud-based rendering using the techniques outlined in the next section
to run in $\mathcal{O}(\frac{\log{N}}{p})$ instead, which is a big improvement.

[fn:transformation_composition] We transform each point twice: Once with the view transformation to render the current point to the canvas in relation to the camera,
and once with the randomly chosen mapping to transform the current point to the next point.
This is what gives rise to the factor $2$.
[fn:definitions] Using the definitions of autoref:subsection:chaos_game: $N$ is the total number of points we want to render in our attractor approximation, 
and $x$ is the minimum number of transformations we need to apply to any arbitrary point to make it visually indistinguishable from a point exactly on the attractor.

*** Potential point cloud-based optimizations
\label{subsection:point_cloud_optimizations}

The generation and rendering of point clouds is a quite well-understood problem[CITE]. point clouds see widespread use,
most commonly in 3D-graphics that originates from a '3D scanner' .

point clouds can be rendered in a reasonably efficient manner by storing them in a 'Bounding Volume Hierarchy',
for instance in a binary search tree that is ordered using the Morton space filling curve. [CITE]
Storing the points of a point cloud in this way allows us to efficiently cull most uninteresting points (i.e. points that would end up outside of the current camera viewport),
which speeds up the rendering procedure tremendously.

However, while this problem is well-understood, the implementation is far from trivial [CITE].

*** Detecting self-similarity: 'Zooming In by Zooming Out'
\label{subsection:self_similarity}

When using a point-cloud, we retain detail when zooming in up to a certain depth. In this way, a point cloud is more flexible than a 
static pixel canvas, which will already show rendering artefacts when zooming in slightly beyond its intended size.
\todo[inline]{More info}

Nonetheless, beyond a certain depth, the number of points of the point cloud that fall outside of the current camera viewport
(and thus are 'useless' for the quality of the rendered attractor) grows larger and larger.

However, it follows from the self-similar nature of the IFS that we might, in certain situations,
'secretly' zoom out to a shallower camera viewport of the point cloud that shows the same information of the attractor
as the original viewport, but containing more points of the point cloud.

THis can be done by identifying a mapping that fully encompasses the current camera viewport, and then applying its inverse
to the viewport.
'Fully encompasses' here means that the unit square transformed by the mapping fully encompasses
the unit square transformed by the inverse of the camera viewport transformation [fn:fully_encompassing].

\todo[inline]{Example picture}


[fn:fully_encompassing] A simple way to do this is to treat the unit square as a simple polygon,
and then transform all of its corner points. For the resulting two polygons, the 'even-odd rule' algorithm
[CITE]
can be used to check whether all points of one polygon are inside the other. 

*** Coloring the rendering
\label{subsection:coloring}

The simplest way of rendering an IFS attractor simply renders points that are on the attractor a different colour
than the points that are not.

However, more visually pleasing methods use a /color map/ to e.g. indicate the density (the number of points ending up at a particular canvas location) of the attractor.
Yet more advanced methods [CITE fractal flame] keep track of a per-point colour, based on the sequence of transformations it has undergone.

It seems possible to combine these techinques with the 'self-similarity jumping', since we keep track of which mappings we've (inversely) applied to the camera viewport:
to determine the final colors of all points, all visible points need to be multiplied 
all points's colors need to be altered by the color-mutations that each of the inversely-applied mappings would apply.
Essentially, say we are viewing the lower left leaf of a fractal fern, whose mapping would make the contained points reddish. 
If we now 'jump up' (so we use points from virtually the whole fern), we have to alter all points so they get the same reddish hues.


** Implementation

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/program_execution}
  \caption{Overview of the proof-of-concept program's execution flow.}
\label{figure:program_flow}
\end{figure}

The program was implemented using the general-purpose programming language Haskell, 
in combination with the GPGPU library Accelerate [CITE].
This programming stack was chosen because Accelerate 
offers a statically-typed EDSL[fn:EDSL] to array-based GPGPU programming,
which is more high-level and less error-prone than writing shader code in e.g. CUDA or OpenCL directly.[fn:debugging]

The usage of Haskell as implementation language also allowed 
the easy construction of different subcomponents making up the program,
and testing each of these independently, being a pure functional language.

A general overviwe of the flow of the program can be seen in autoref:figure:program_flow.

*** Simplicity

To be able to complete the implementation within the time allotted for the thesis project,
the decision was made to keep the implementation as simple as possible.

This means that 

- the optimizations mentioned in autoref:subsection:point_cloud_optimizations were not implemented.
- points are rendered on screen using a simple binary mapping (if a pixel contains one or more points, it is white; otherwise black.)
  the more fancy coloring techniques outlined in autoref:subsection:coloring were not used.

This means that while the program cannot on its own fully answer the question of whether
this technique is fast enough for real-time usage in all circumstances,
it is able to answer the simpler question of whether the technique of using a point cloud
in combination with 'self-similarity jumping' is at all feasible.

*** Command-line options
\todo[inline]{Write}

*** `.ifs' file format

The configuration language 'Dhall'[CITE] was used to 
easily faciltate the specification of different IFSs.

The file structure allows one to indicate a list of affine transformations with associated chaos game probabilities,
as well as an 'initial camera viewport transformation'. autoref:subsection:viewport_transformation

Dhall allows the definition and re-use of variables, which can be useful
for numerical constants that are used in multiple transformations.[fn:floating_point]

An example file can be seen in autoref:listing:barnsley_fern_ifs_file .


\begin{lstlisting}[float, language=Haskell, frame=single, breaklines=true, basicstyle=\scriptsize\tt, captionpos=b, caption={barnsley\_fern.ifs, representing \autoref{ifs:barnsley_fern}}, label={listing:barnsley_fern_ifs_file}]
{ initialCamera =
  { a = 9.090909090909091e-2
  , b = 0.0
  , c = 0.0
  , d = -9.090909090909091e-2
  , e = 0.5
  , f = 1.0
  }
, transformations =
  [ { transformation = { a = 0.0, b = 0.0, c = 0.0, d = 0.16, e = 0.0, f = 0.0 }
    , probability = 1.0e-2
    }
  , { transformation = { a = 0.85, b = 4.0e-2, c = -4.0e-2, d = 0.85, e = 0.0, f = 1.6 }
    , probability = 0.85
    }
  , { transformation = { a = 0.2, b = -0.26, c = 0.23, d = 0.22, e = 0.0, f = 1.6 }
    , probability = 7.0e-2
    }
  , { transformation = { a = -0.15, b = 0.28, c = 0.26, d = 0.24, e = 0.0, f = 0.44 }
    , probability = 7.0e-2
    }
  ]
}

\end{lstlisting}

[fn:floating_point] Unfortunately, Dhall explicitly does not allow floating-point arithmetic.[CITE]
As such, one still needs to write e.g. $1/3$ as $0.3333333333333333$.

*** Rendering
The program computes the point cloud once, on startup, and then re-renders the image that is shown in the program window
every time the user changes the camera viewport.

Rendering is done by iterating (in parallel) over all points in the point cloud and filling a two-dimensional histogram with the same dimensions
as the canvas with numbers. 
This histogram is then used to draw the attractor (any non-empty pixel is colored white and the rest black).

More sophisticated rendering techniques are possible (see autoref:subsection:coloring), 
but not implemented in the program.

*** Changing the camera viewport
Changing the camera viewport can be done by either zooming in or out using the scrollwheel,
or dragging with the mouse to translate the camera viewport.

The camera's view transformation itself is stored as a transformation matrix relative to the unit square.

*** Manually performing 'self-similarity jumping'

While the program is running, a user can go back to a more shallow view by pressing `+`[fn:plus], and then when inside one or multiple shallower views,
`-` can be pressed to undo the last jump.

Care is taken to only allow the jump up if the current camera viewport is fully contained within one mapping's region.

That this process is kept manual was intentional, because it allows the user to more easily compare how
the representation looks with and without the jumping, and allows for a full exploration of the circumstances
in which a jump up is and is not actually correct (see autoref:subsection:jumping_restrictions).

[fn:plus] Strictly speaking, by pressing the `=` key; pressing SHIFT is not necessary.
*** Rendering 'guides'

To make it easier to see how an IFS is constructed, as well as easier for a user to orient themselves when
testing the 'self-similarity jumping', it is possible to toggle the rendering of 'guides' by pressing the `g` key.
Similarly, the rendering of points can be toggled by pressing the `p` key (allowing one to see the guides more clearly, when desired).

These 'guides' are the unit squre, after undergoing a sequence of zero, one, two etc. mappings of the IFS.
Different colours are used for guides at different sequence-depths.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_points}
         \caption{only points}
         \label{figure:barnsley_guides}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_guides_and_points}
         \caption{guides and points}
         \label{figure:barnsley_guides_and_points}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_guides}
         \caption{only guides}
         \label{figure:barnsley_guides}
     \end{subfigure}
        \caption{The Barnsley Fern (\autoref{ifs:barnsley_fern}) rendered by the program in different ways.}
        \label{figure:barnsley_guides_vs_points}
\end{figure}

[fn:EDSL] Embedded Domain-Specific Language.
[fn:debugging] Instead of being presented with a black screen when a programming mistake is made, 
Accelerate presents errors at compile-time in many cases. Furthermore, Accelerate features a
single-threaded reference implementation that runs on the CPU that can be used to sanity-check the behaviour of code.

* Findings
\label{section:findings}

** Restrictions on replacing the view with a more shallow view
\label{subsection:jumping_restrictions}

From experimentation with the program it turns out that there are two common situations
in which the technique of replacing the camera viewport with a more shallow camera viewport that is outlined in autoref:subsection:self_similarity cannot be used.

**** Borders between transformations

It is rather common to zoom in on the borders between transformations, as this is often
where interesting visual details of the IFS might appear.

However, the algortihm as outlined in [REF EARLIER ALGORITHM] is not able to handle borders between transformations,
thus making it useless in these scenarios.

**** Overlapping subtransformations

A more shallow view of the attractor only shows the same as a deeper view when
there are no points transformed by another mapping that end up in the deeper view.

When there are points from another mapping in the current view, 
going to a more shallow view will make points 'disappear' from the perspective of the user.
In practice, this means that for many IFSs there are large regions in which the technique cannot be used at all.

Simple IFSs like the SierpinskÃ½ Triangle[REF], in which transformations
do not overlap, do not exhibit this problem. 
Slightly more complicated IFSs like the Dragon Curve[REF] or the Barnsly Fern[REF] however do.
See autoref:figure:dragon_curve_overlaps for an graphical explanation.

This case is annoyingly common and there is no clear solution to alleviate this restriction.
What is more, it is not simple to check whether we are currently in a region that exhibits the problem,
as this would require evaluating the IFS itself.

It is possible to take a rough 'upper bound' estimate of the disallowed regions by keeping track, 
per mapping, where the unit square would end up after a couple of mappings with this mapping as last (i.e. most significant) one.


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_a}
         \caption{The unit square (indicated in white) transformed by the two mappings of $D$ (indicated in green)}
         \label{figure:dragon_curve_a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_b}
         \caption{The first of the two mappings, transformed one more time by the mappings of $D$ (indicated in red)}
         \label{figure:dragon_curve_b}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_c}
         \caption{The second of the two mappings, transformed one more time by the mappings of $D$ (indicated in blue)}
         \label{figure:dragon_curve_c}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_d}
         \caption{The region in which (b) and (c) overlap (indicated in magenta)}
         \label{figure:dragon_curve_d}
     \end{subfigure}
        \caption{Showing the first couple of iterations of rendering the attractor of the dragon curve IFS $D$ (\autoref{ifs:dragon_curve}), and the regions in which (sequences of) transformations overlap.}
        \label{figure:dragon_curve_overlaps}
\end{figure}

\todo[inline]{pseudocode}


# Zooming out is possible iff
# - The camera is fully contained inside the transformation
# - The camera is not overlapping any of the other transformations' 'immediate child-transforms'

# Further restriction: when viewport overlaps multiple transformations there is a problem, 
# even if this would theoretically be a possibility. Maybe there are smarter ways of recognizing self-similarity?

** Memory Usage

Point clouds take up a lot of data on the GPU. To render a fractal at reasonable detail, 
hundreds of millions if not billions of points are necessary (depending on the particular IFS).

A reasonable way to store a point cloud is by using for each 2D-point, 32 bits for each coordinate, thus fitting the pair in exactly one machine word of 64-bit systems.
Stored this way, a point cloud of 100,000,000 points requires 0.596 GiB of GPU memory,
and 1,000,000,000 points requires 5.96 GiB.
For current generation GPUs[fn:GPU2020], this often is more memory than available.

[fn:GPU2020] At the time of writing, high-end consumer GPUs contain somewhere between 2 and 24 GiB of available memory. [CITE]
CITE: https://www.ign.com/articles/the-best-graphics-cards-3

* Conclusion
\label{section:conclusion}

A program was implemented which has shown that there is /some/ merit to rendering an IFS' attractor using a point-cloud as re-usable intermediate structure.
However, the self-similarity detection method that was proposed turns out to be unusable in common cases.
Furthermore, self-similarity 'jumps' make more sophisticated rendering techniques difficult if not impossible to use.

As long as these two problems remain unsolved, the proposed technique can only be considered impractical.

* Further Work
\label{section:further_work}

It is our hope that a more sophisticated way of detecting self-similarity might be found,
which would make 'self-similarity jumping' more practical.

Besides this, while we have shown in a proof-of-concept program that it is possible to render an IFS using a point cloud
with a reasonable speed, there are many optimizations that could be made to make the program run faster (potentially even in real-time),
most notably the rendering optimizations listed in autoref:subsection:point_cloud_optimizations

Another venue that could be explored is the rendering of an IFS' attractor at multiple 'levels of detail':
It might be possible to create more detailed local versions of the point cloud (based on the points of the less detailed point cloud) when the user
zooms in on a particular region, on demand.

Finally it is worth noting that as mentioned in autoref:subsection:deterministic_gpu, [CITE] already presents an efficient way to render a large set of IFSs using a very different approach, 
which might be worthwhile to explore further.

\appendix

* IFSs used

This appendix lists the transformation matrices of the IFSs that were used throughout this thesis.

\todo[inline]{Sierpi\'nsky}
\todo[inline]{Dragon Curve}
\todo[inline]{Barnsley Fern}

\label{ifs:sierpinsky}
\label{ifs:barnsley_fern}
\label{ifs:dragon_curve}
