#+TITLE: \Huge In by Out Again
#+SUBTITLE: Arbitrarily-Deep Zooming on Iterated Function Systems by `Self-Similarity Jumping'

#+BIND: org-latex-prefer-user-labels t

#+LATEX_HEADER: \setlength{\parindent}{1em}
#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[citestyle=alphabetic,bibstyle=alphabetic, hyperref=true, backref=true,maxcitenames=3,url=true,backend=biber,natbib=true] {biblatex}
#+LATEX_HEADER: \addbibresource{bibliography.bib}

#+LATEX_HEADER: \usepackage[a4paper, total={7in, 9in}]{geometry}
#+LATEX_HEADER: \usepackage[font={small, it},labelfont=bf]{caption}

#+LATEX_HEADER: \usepackage[ruled, procnumbered]{algorithm2e}
#+LATEX_HEADER: \usepackage{listings}

# not emph
#+LATEX_HEADER: \SetArgSty{}

#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{pifont}
#+LATEX_HEADER: \newcommand{\cmark}{\color{ForestGreen}\ding{52}}%
#+LATEX_HEADER: \newcommand{\xmark}{\color{Maroon}\ding{55}}%

#+LATEX_HEADER: \hypersetup{colorlinks=true}

#+LATEX_HEADER: \renewcommand{\sectionautorefname}{{\color{Black}\S}}
#+LATEX_HEADER: \renewcommand{\subsectionautorefname}{{\color{Black}\S\S}}
#+LATEX_HEADER: \renewcommand{\subsubsectionautorefname}{{\color{Black}\S\S\S}}
#+LATEX_HEADER: \renewcommand{\functionautorefname}{{\color{Black}\textbf{Function}}\color{Magenta}}
#+LATEX_HEADER: \renewcommand{\algorithmautorefname}{{\color{Black}\textbf{Algorithm}}\color{Magenta}}


#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[shortlabels]{enumitem}

#+LATEX_HEADER: \usepackage{newfloat}
#+LATEX_HEADER: \DeclareFloatingEnvironment[fileext=lol, listname={List of Iterated Function System definitions}, name=IFS, placement=tbhp, within=section]{ifs}

#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{todonotes}

#+LATEX_HEADER: \usepackage{pifont,kantlipsum}
#+LATEX_HEADER: \newcommand*{\altasterism}{\vspace*{1em plus .5em minus .5em}\noindent\hspace*{\fill}\ding{104}\hspace*{\fill}}



#+LATEX_HEADER: \usepackage{rugscriptie}
#+LATEX_HEADER: \supervisor{dr. J. Kosinka}
#+LATEX_HEADER: \supervisor{G. J. Hettinga}
#+LATEX_HEADER: \date{August 2020}
#+LATEX_HEADER: \faculty{fse} % Or feb, fgg, fgmw, fl, frg, frw, fw, umcg
#+LATEX_HEADER: \thesistype{Bachelor's thesis} % Will be printed unmodified

#+OPTIONS: toc:4

\listoftodos

\todo[inline]{Turn off todo list}

\pagebreak

* Abstract
  :PROPERTIES:
  :UNNUMBERED: notoc
  :END:

\todo[inline]{Consider improving formulation of IFSs}

Iterated Function Systems (IFSs) are a mathematical approach to rendering fractals that sees wide usage in the modeling of physical phenomena, 
image compression, and the creation of abstract art.
When exploring an IFS interactively, current IFS rendering techniques require a full re-approximation of the IFS's attractor at every frame.

This thesis proposes a combination of two techniques to enable faster exploring of IFSs.
First, a point cloud is used as an intermediate attractor approximation, that can be re-used between animation frames.
Secondly, a technique coined 'self-similarity jumping' is proposed to keep the attractor representation detailed, even when zooming in very far.

A proof-of-concept computer program has been implemented
which shows that the employed techniques, while promising, 
are somewhat restricted in their usefullness because self-similarity jumping cannot be used in all situations.

\pagebreak

* Introduction

\todo[inline]{Consider improving formulation of IFSs}

Iterated Function Systems (IFSs) are a method to generate infinitely detailed fractal images 
by repeatedly applying simple mathematical functions until a fixed point is reached cite:barnsley1988fractals. 
IFSs see use in the modeling and rendering of physical phenomena, image compression cite:hart1996fractal and representing gene structures cite:jeffrey1990chaos.
Sometimes they also are used plainly for the aesthetic beauty of their graphical representations cite:draves2003fractal.

Various computer algorithms to visualize IFSs exist cite:hepting1991rendering.
However, these all take a single still image as final result. If these algorithms are employed to render an animation,
this animation is treated as a sequence of completely separate still images.

This leaves a venue for potential optimization: if there is information that remains the same between animation frames, 
then we could compute this information only once and re-use it for all frames.

For instance, many kinds of animations consist of transformations of the camera viewport w.r.t the viewed fractal over time like translation, rotation and scaling. 
These transformations do not require alterations to the fractal itself.
This means that (an approximation of) the fractal might be computed once and then be used for all frames.

Furthermore, because of the self-similar nature of the rendered fractals,
it might be possible to simulate zooming in to an arbitrary depth by `jumping up' to a more shallow camera viewport
that shares the same self-similarity as the original camera viewport.

This thesis is an in-depth investigation of these two ideas.

** Overview

In the next section, autoref:section:background, Iterated Function Systems are introduced and pre-existing methods to render their attractors in single-threaded and heavily parallel environments described.
This then leads to a clear definition of the research question in autoref:section:research_question.
The approach taken to test this question is described in autoref:section:approach, followed by a qualitative discussion of the results in autoref:section:findings.
We conclude in autoref:section:conclusion and finally hint at some approaches for further work in autoref:section:further_work.

* Background
\label{section:background}

This section describes the different building blocks necessary to formulate the research question.
First, IFSs are formalized, followed by a description of the different ways in which an IFS's attractor can be rendered, 
and how these techniques might be parallellized.

** Informal description of an Iterated Function System
\label{subsection:informal_description}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/sierpinsky_iterations}
\caption{The first six iterations of the Sierpi\'nsky triangle IFS (\autoref{ifs:sierpinsky}). 
The initial image is just the unit square. We then iteratively combine the results of transforming the current image using one of the three mappings. 
The letters indicate which (sequence of) transformation(s) resulted in this part of the image.
Dashed red lines are used for the first four iterations to indicate the self-similarity between the previous iteration and the current one extra clearly.
Already after a couple of iterations it can be seen that the shape of the original image has no influence on the shape of the attractor.}
\label{figure:sierpinsky_iterations}
\end{figure}


Informally, an Iterated Function System is a set of mappings (transformation functions) that, given any input image, create a new image by

1. transforming the input image with each of the transformations;
2. overlaying all transformed images, forming one new image that is the combination (union) of the transformed images. This is the new image.

This process is then repeated an arbitrary number of times, until changes between the input image and new image are no longer visible to the human eye.

The image you end up with is a visual representation of the IFS's attractor.
A simple example of this process can be seen in autoref:figure:sierpinsky_iterations.



** Formal definition of an Iterated Function System

Formally, an Iterated Function System $F$ consists of a finite set of contraction mappings that map a complete metric space[fn:definition_metric_space] $(\mathcal{M}, d)$ to itself cite:barnsley1988fractals:

$$ \mathcal{F} = \{ f_i : \mathcal{M} \rightarrow \mathcal{M}\}_{i=1}^N, N \in \mathbb{N}.$$

All mappings are required to be contractive. This means that for each mapping $f_i$, the distance between every two arbitrary points $a$ and $b$ in $(\mathcal{M}, d)$ 
needs to be larger than the distance of these points after transforming them:

$$\forall i \bigl( d(f_i(a), f_i(b)) < d(a, b) \bigr).$$

We can then take the union of performing all of these mappings on any compact set of points $\mathcal{S}_0 \subset \mathcal{M}$. This procedure is called the /Hutchkinson Operator/ and denoted $H$.
It can be iterated as many times as desired:

$$ \mathcal{S}_{n + 1} = H(\mathcal{S}_n) = \bigcup_{i=1}^{N} f_i(\mathcal{S}_n), n \in \mathbb{N}.$$

When performed an arbitrary number of times, the fixed-point or attractor, $\mathcal{A}$, of $\mathcal{F}$ is approached:

$$\mathcal{A} = \lim_{n \rightarrow \infty} \mathcal{S}_n.$$

Curiously, which set of points $\mathcal{S}_0$ we picked does not influence the shape of $\mathcal{A}$ cite:mendivil2003fractals. We might even start with a single point (denoted $z_0$).

[fn:definition_metric_space] A metric space is a set $\mathcal{M}$ together with a /metric/ $d(x, y)$ on that set . The metric is a function that for any two elements (or `points') in $\mathcal{M}$ returns the `distance' between them, for any notion of distance adhering to the `identity of indiscernibles' ($d(x, y)  = 0 \Leftrightarrow x = y$), `symmetry' ($d(x, y) = d(y, x)$) and `triangle inequality' ($d(x, z) \leq d(x, y) + d(y, z)$) properties. Often, $d$ is elided and just $\mathcal{M}$ is used to refer to the metric space when it is clear from context which metric is used.

*** Restriction to affine transformations on the two-dimensional Euclidean plane 

\todo[inline]{Improve info about restrictions (c.f. feedback)}

Most research of IFSs restricts itself to using $\mathbb{R}^2$ as metric space[fn:euclidean] which can easily be rendered to screen or paper.
Furthermore, most commonly-used IFSs only use /affine transformations/ as mappings.

It is very practical to work in this restricted domain 
and potentially generalize obtained results to a wider domain later.
Therefore, these are also the restrictions that will be used in this thesis.

*** The viewport transformation
\label{subsection:viewport_transformation}

When rendering graphics, we view the world through a (virtual) /camera/ which has a particular frustum 
that limits what parts of the world (in this case the IFS's attractor) end up on the /viewport/.

**** Scaling vs zooming

Because of the presence of a camera, the part of an object that will be visible in the camera viewport may change when scaling said object.
We use the term `zooming' to disambugate this type of scaling where a camera is present.

**** Freedom of choosing an initial camera position and frustum

For any IFS we can transform its attractor by any invertible map $t$ by adjusting each of the IFS's mappings according to the
transform theorem, defined as $f_i' = t \circ f_i \circ t^{-1}$ cite:barnsley1988fractals. 
Essentially points are transformed from the new (program-desired) space to the old (user-supplied) space, then the mapping is applied, and finally the points are transformed back to the new space.
This allows users the freedom to choose any desired mappings together with an `initial camera transformation' (i.e. the camera's initial position + frustum),
while still allowing all calculations to happen with regard to the unit square (`unit space'), keeping them simple.

[fn:euclidean] More formally, the two-dimensional Euclidean space: $\left(\mathbb{R}^2, d(p, q) = \sqrt{(p - q)^2}\right)$.

** Rendering an Iterated Function System

A couple of algoritms (cite:barnsley1988fractals, cite:hepting1991rendering, cite:lawlor2012gpu) exist to render the attractor of an Iterated Function System. 
It is impossible to render the attractor exactly, as this would require an infinite number of transformation steps.
However, we can approximate it until the difference between our approximation and the attractor is smaller than
the smallest detail we can visually represent (e.g. when rendering to a screen, smaller than the size of a pixel).

Because we apply $H$ many times and each time consists of taking the union of $N$ different transformations,
the result can be seen as traversing an (infinitely deep) tree of transformations, 
where each sub-tree is self-similar to the tree as a whole.

\todo[inline]{Define trees (in a footnote?)}

Different algorithms take different approaches to evaluating this tree up to a chosen finite depth.

More in-depth information about the rendering of Iterated Function Systems can be found in cite:hepting1991rendering. 
Short summaries of the two most common techniques now follow.

*** The deterministic method

In this approach we evaluate the whole tree up to a chosen depth. The algorithm works as follows:

1. Pick a starting point $z_0$;
2. traverse the tree down to the chosen depth $k$, keeping track of the traversed sequence of transformations [fn:function_composition]
   $f_{i_k} \circ \ldots \circ f_{i_1}$;
3. for each node at this depth, evaluate and render $z_k = (f_{i_k} \circ \ldots \circ f_{i_1})(z_0) = f_{i_k-1}(z_{k-1})$.

Since $z_{k} = f_{i_k-1}(z_{k-1})$ this procedure takes, for an approximation that consists of $P$ points, depending on the tree traversal chosen:

- a linear amount of memory ( $\mathcal{O}(P)$ ) for a breadth-first tree-traversal;
- a logarithmic amount of memory ( $\mathcal{O}(\log{P})$ ) for a depth-first tree-traversal.

\todo[inline]{Explain why. Or refer to external resource?}

The advantage of the breadth-first traversal is that the rendering process can be stopped interactively,
while the depth-first traversal requires the stopping criterion to be known beforehand cite:hepting1991rendering.

Both kinds of traversals take a linear amount of time ( $\mathcal{O}(N \cdot P) \approx \mathcal{O}(P)$, where $N$ is the number of mappings the IFS consists of).

While the deterministic method is easy to understand (and indeed is a direct implementation of the informal process described in autoref:subsection:informal_description),
it is usually less efficient and more complex to implement on a computer than the algorithm that is described next.

\todo[inline]{Double-check pre/postmultiplication problems}

[fn:function_composition] $\circ$ stands for function composition: $(f \circ g)(x) = f(g(x))$. 
Be aware that when affine transformation functions are represented as matrices (e.g. $F$ and $G$), the matrix premultiplication resulting in the same transformation is in the opposite order ($f \circ g \Leftrightarrow G \cdot F$). Matrix postmultiplication cannot be (easily) used in all of the presented algorithms, c.f. cite:hepting1991rendering.

*** The chaos game
\label{subsection:chaos_game}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_1000000}
         \caption{1,000,000}
         \label{figure:barnsley_mil}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_100000000}
         \caption{10,000,000}
         \label{figure:barnsley_ten_mil}
     \end{subfigure}
        \caption{The Barnsley Fern (\autoref{ifs:barnsley_fern}), rendered using the chaos game with different numbers of points.}
        \label{figure:barnsley_chaos_game_points}
\end{figure}


The /stochastic method/ cite:hepting1991rendering, also known as the /random iteration algorithm/ cite:barnsley1988fractals or more frequently the /chaos game/, works as seen in autoref:chaosGame.

\begin{algorithm}
\caption{the chaos game}
\label{chaosGame}
  $N$: the number of mappings of the IFS; \\
  $z$: a single arbitrary starting point; \\
  $v$: the camera's view transformation; \\
  $m = 0$; \\
  \For{$m \in [0..n + P)$}{ 
    $i$: a random integer between $0$ and $N$;  \\
    \If{$m \geq n$}{
      render($v(z)$) cumulatively; \\
    }
    $z = f_i(z)$; \\
  }

\end{algorithm}

This method converges to a correct result because of the following two facts:

- because the precision of the canvas we render on is finite, and because all transformations are contracting,
 two points $a$ and $b$ are indistinguishable after only $n$ transformations.
  In other words, only the latest $x$ transformations determine at what location on the canvas a point will end up (with the latest transformation having the largest effect on the point's final location).[fn:contraction]
- at each depth in the tree the subtree remains the same, so every sequence of transformations approaches the attractor.

Therefore, all intermediate points after the first $n$ iterations are visually indistinguishable from a point that is part of the attractor.
By running this non-deterministic approach for sufficiently many iterations we approach a diverse enough set of 'transformation sequences of length $n$' that we end up covering the whole attractor.

The nice thing about the chaos game is that it requires only a constant amount of auxiliary memory, so its memory complexity is $\mathcal{O}(1)$.
Furthermore, its time complexity is similar to the deterministic method but with a smaller constant factor, at $\mathcal{O}(2(P + n)) \approx \mathcal{O}(P + n)$. [fn:linear_time_chaos_game]

A disadvantage of the chaos game is that the result is by its very nature /non-deterministic/.
If not enough points are used, the result might end up `grainy' and it is not predictable what part of the attractor will be covered (see autoref:figure:barnsley_chaos_game_points).

One further disadvantage, is that in its simplest form, all mappings have an equally likely chance to be used.
However, because some mappings might be (much) more contracting than others, this means that coverage of the attractor is not even,
which means that we need to use many more iterations.

Therefore, most implementations of the chaos game allow the user to specify for each mapping a /probability/ that it is used. 
When highly contracting mappings are chosen less frequently, coverage of the attractor will be even. [fn:probabilities]

\altasterism

Because of its simplicity and computational efficiency, the chaos game is used more frequently than the deterministic method for practical implementations.
The chaos game is also easier to parallellize for Graphical Processor Units (GPUs), as will be outlined in the next subsection.

[fn:contraction] Methods for precisely determining the lower and upper bounds of IFS contraction for a particular IFS (and therefore the exact value of $n$) exist cite:hepting1991rendering, 
but are not relevant for this thesis.

[fn:probabilities] These probabilities are often fine-tuned by hand, although algorithms to determine balanced probabilities exist as well cite:hepting1991rendering.

[fn:linear_time_chaos_game] which again is $\approx \mathcal{O}(P)$ when $n << P$ which is often the case.

** Parallellizing IFS rendering by using a Graphical Processor Unit

It is enticing to port IFS rendering to run on a Graphical Processor Unit (GPU) because to produce a smooth image, hundreds of millions of points are often needed.

However, optimizing IFS rendering to run well on GPU-architectures is a bit of a challenge.

GPU shaders usually operate by running a check for every pixel on the final texture (i.e. canvas), to determine its color.
For other fractals like the Mandelbrot- and Julia-sets, this is a natural fit since the construction of those fractals works exactly in that way.

For an IFS this does not work, as an IFS is created in the other direction. Points end up at some location on the canvas only after transforming many times.
Attempts to go the other way fall flat, for instance because this would require to invert the IFS's mappings, but they are not guaranteed to be invertible.

Instead, General-Purpose GPU-programming (GPGPU) techniques have to be employed, as these are able to use the top-down approach.

*** The chaos game on the GPU
\label{subsection:chaos_game_gpu}

The (classical) deterministic method is difficult to parallellize on the GPU because of the extra memory that is required to keep track of the current position in the tree.
Coordinating which GPU thread would calculate which part of the tree and sharing results would be a hassle.

Instead, the chaos game is more frequently used because of its simplicity. 
It is parallellized in a straightforward way, 
by running the iteration process many times side-by-side (one iteration process per GPU thread),
and then combine the final results of all of these on a single canvas. cite:green2005gpu

*** The deterministic method on the GPU
\label{subsection:deterministic_gpu}

An exciting approach taken in cite:lawlor2012gpu /does/ use the deterministic method instead:
by using the fast inverse square root operation together with a few other tricks, 
even unbounded (noncontracting) and nonlinear IFSs can be efficiently
evaluated using the deterministic method, programmed in normal GPU shaders that manipulate a couple of GPU textures.

* Research Question
\label{section:research_question}

In the last section, the construction of an IFS's attractor was formally defined, 
and different approaches of rendering were outlined.

While many different approaches to IFS rendering exist, some of them quite efficient,
none re-use information from rendering one image of the IFS for the rendering of another.

This leads us to the research question of this thesis:

**Is it possible, by re-using information between animation frames, 
to render animations of an Iterated Function System's attractor in which the camera zooms in, in real-time?**

* Approach
\label{section:approach}

To put this to the test, a simple software program was created which calculates the IFS's attractor only once,
and then allows a user to interactively zoom and pan the camera around to investigate different parts of the attractor.

** Design

The inspiration of the design is two-fold:

First, we use the insight that the (parallel) chaos game can be used to generate a /point cloud/, allowing us to re-use parts of the computation between animation frames
and thus render each frames faster.

Second, while zooming in on a point cloud only works up to a particular depth before losing considerable detail, 
it is possible to detect when we are looking at a self-similar part of the attractor.
This allows us, in many situations, to replace the current camera viewport with a more shallow one, 
keeping the amount of detail high.

*** Point clouds

The main inspiration for the re-usability approach is that we can modify the GPU-variant of the chaos game algorithm outlined in autoref:subsection:chaos_game_gpu
to render to a /point cloud/ instead of immediately to a canvas.
When we then move around the camera, we are able to re-use the points in the point cloud;
only where the points in the point cloud end up on screen exactly needs to be re-calculated, 
by transforming all of the points exactly once with the camera's `view transformation'.

This is faster than re-evaluating the whole attractor using the chaos game at every frame which would require transforming all points /many/ times.

Formally, to render an attractor approximation consisting of $P$ points, 
running the whole chaos game each frame takes $2(P+n)$ transformations per frame (c.f. autoref:subsection:chaos_game). 
Running this on $p$ parallel threads has a time complexity of $\mathcal{O}(\frac{2(P+pn)}{p})$.

Unoptimized, it takes $P$ transformations to render a precomputed point cloud to screen each frame 
(paralellized this corresponds to a time complexity of $\mathcal{O}(\frac{P}{p})$).
This does not seem very impressive since $\mathcal{O}(\frac{2(P+pn)}{p}) \approx \mathcal{O}(\frac{2P}{p}) \approx \mathcal{O}(\frac{P}{p})$,
placing the two approaches in the same order of efficiency. However, it is possible to optimize point cloud-based rendering using the techniques outlined in the next section
to run in $\mathcal{O}(\frac{\log{P}}{p})$ instead, which is a big improvement.

[fn:transformation_composition] We transform each point twice: Once with the view transformation to render the current point to the canvas in relation to the camera,
and once with the randomly chosen mapping to transform the current point to the next point.
This is what gives rise to the factor $2$.

*** Potential point cloud-based optimizations
\label{subsection:point_cloud_optimizations}

The generation and rendering of point clouds is a quite well-understood problem cite:wimmer2006instant. point clouds see widespread use,
most commonly in 3D-graphics that originates from a `3D scanner'.

Point clouds can be rendered in a reasonably efficient manner by storing them in a `Bounding Volume Hierarchy',
for instance in a binary search tree that is ordered using the Morton space filling curve. cite:lauterbach2009construction
Storing the points of a point cloud in this way also allows us to efficiently cull most points that would end up outside of the current camera viewport,
which speeds up the rendering procedure tremendously.

However, while this algorithm is well-understood, the implementation is far from trivial cite:lauterbach2009construction.

*** Self-similarity jumping: `zooming in by zooming out'
\label{subsection:self_similarity}

\begin{figure}

\includegraphics[width=\textwidth]{figures/sierpinsky_jump}
\caption{An example of the self-similarity jumping technique. Pictured is the Sierpi\'nsky triangle IFS (\autoref{ifs:sierpinsky}).
The viewport (pictured in cyan) on the left can be transformed to the one on the right by applying the inverse mapping $f_1^{-1}$ to it.
The resulting viewport looks the same but contains more points.}
\label{figure:sierpinsky_jump}
\end{figure}

When using a point cloud, we retain detail when zooming in up to a certain depth. In this way, a point cloud is more flexible than a 
static pixel canvas, which will already show rendering artefacts when zooming in slightly beyond its intended size.

Nonetheless, while zooming in, more and more points of the point cloud fall outside of the current camera viewport
(and thus are 'useless' for the quality of the rendered image), degrading quality beyond a certain depth more than is acceptable.

However, it follows from the self-similar nature of the IFS that we might, in certain situations,
`unnoticeably' zoom out to a shallower camera viewport of the point cloud that shows the same information of the attractor
as the original viewport, but containing more points of the point cloud.

This can be done by identifying a mapping that fully encompasses the current camera viewport, and then applying its inverse
to the viewport.
`Fully encompasses' here means that 
all corners of the unit square transformed by the inverse of the camera viewport transformation
lie inside of the unit square transformed by the mapping [fn:fully_encompassing].

See autoref:figure:sierpinsky_jump for an example. 

The algorithm and its inverse are specified in autoref:algorithm:self_similarity_jump_up and autoref:algorithm:self_similarity_jump_down.

\begin{algorithm}
\caption{self-similarity jump-up}
\label{algorithm:self_similarity_jump_up}
  $n$: the number of mappings the IFS consists of. \\
  $v$: the current camera's view transformation. \\
  $s$: a stack of jumps made so far. \\
  \For{$i \leftarrow [1..n]$}{ 
    \If{isInvertible($f_i$)  and isInside($v^{-1}$, $f_i$) }{
      push($s$, $f_i$) \\
      $v = f_i^{-1}(v)$ \\
      break \\
    }
  }
\end{algorithm}

\begin{algorithm}
\caption{self-similarity jump-down}
\label{algorithm:self_similarity_jump_down}
  $u$: the identity transformation \\
  $v$: the current camera's view transformation. \\
  $s$: a stack of jumps made until now. \\
  \If{notEmpty($s$) and isOutside($v^{-1}$, $u$) }{
    $f$ = pop($s$) \\
    $v = f(v)$ \\
  }
\end{algorithm}



[fn:fully_encompassing] A simple way to do this is to treat the unit square as a simple polygon,
and then transform all of its corner points. For the resulting two polygons, the 'even-odd rule' algorithm
cite:haines1994point
can be used to check whether all points of one polygon are inside the other. 

*** Coloring the rendering
\label{subsection:coloring}

The simplest way of rendering an IFS attractor simply renders points that are on the attractor a different color
than the points that are not.

However, more visually pleasing methods use a /color map/ to e.g. indicate the density (the number of points ending up at a particular canvas location) of the attractor.
Yet more advanced methods cite:draves2003fractal keep track of a per-point color, based on the sequence of transformations it has undergone.

It seems possible to combine these techinques with the `self-similarity jumping', since we keep track of which mappings we've (inversely) applied to the camera viewport:
to determine the final colors of all points that will be rendered this frame, 
all visible points' colors need to be altered by the color-mutations that each of the mappings in $s$ would apply.

As an example, say we are viewing the lower left leaf of a fractal fern (like autoref:ifs:barnsley_fern) and that mapping creating the lower left leaf would make the contained points red. 
If we now `jump up' we use points from virtually the whole fern.
To make these points still look visually identical from the lower left leaf, we have to alter the points' colors so they get the same reddish hues.

** Implementation

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/program_execution}
  \caption{Overview of the proof-of-concept program's execution flow.}
\label{figure:program_flow}
\end{figure}

The program was implemented using the general-purpose programming language Haskell, 
in combination with the GPGPU library Accelerate cite:chakravarty2011accelerating.
This programming stack was chosen because Accelerate 
offers a statically-typed EDSL[fn:EDSL] for array-based GPGPU programming,
which is more high-level and less error-prone than writing code 
in lower-level alternatives like CUDA or OpenCL directly.[fn:debugging]

The usage of Haskell as implementation language, being a pure functional language, also allowed 
the easy construction of different subcomponents making up the program,
and testing each of these independently.

A general overview of the flow of the program can be seen in autoref:figure:program_flow.

*** Simplicity

To be able to complete the implementation within the time allotted for the thesis project,
the decision was made to keep the implementation as simple as possible.

This means that:

- The optimizations mentioned in autoref:subsection:point_cloud_optimizations were not implemented;
- Points are rendered on screen using a simple binary mapping. (If a pixel contains one or more points, it is white; otherwise black.)
  The more fancy coloring techniques outlined in autoref:subsection:coloring were not used.

While the program on its own might therefore not be enough to fully answer the research question,
it is able to answer the simpler question of whether the technique is at all feasible.

*** Command-line options

The proof-of-concept program allows the customization of the following options

- `samples`: the number of points to use for the chaos game (defaults to 100,000,000)
- `parallellism`: the number of GPU-threads to split the number of samples across. (defaults to 2048)
- `seed`: a number to seed the random number generator with. If not provided, a different arbitrary seed will be used each time.
- `render\under{}width` and `render\under{}height` set the resolution of the program window that is displayed (defaults to $800 \times 800$).

*** The `.ifs' file format

The configuration language `Dhall' cite:gonzalez2019 was used to 
easily faciltate the specification of different IFSs.

The file structure allows one to indicate a list of affine transformations with associated chaos game probabilities,
as well as an initial camera view transformation. autoref:subsection:viewport_transformation

Dhall allows the definition and re-use of variables, which can be useful
for numerical constants that are used in multiple transformations.[fn:floating_point]

An example file can be seen in autoref:listing:barnsley_fern_ifs_file .


\begin{lstlisting}[float, language=Haskell, frame=single, breaklines=true, basicstyle=\scriptsize\tt, captionpos=b, caption={barnsley\_fern.ifs, representing \autoref{ifs:barnsley_fern}}, label={listing:barnsley_fern_ifs_file}]
{ initialCamera =
  { a = 9.090909090909091e-2
  , b = 0.0
  , c = 0.0
  , d = -9.090909090909091e-2
  , e = 0.5
  , f = 1.0
  }
, transformations =
  [ { transformation = { a = 0.0, b = 0.0, c = 0.0, d = 0.16, e = 0.0, f = 0.0 }
    , probability = 1.0e-2
    }
  , { transformation = { a = 0.85, b = 4.0e-2, c = -4.0e-2, d = 0.85, e = 0.0, f = 1.6 }
    , probability = 0.85
    }
  , { transformation = { a = 0.2, b = -0.26, c = 0.23, d = 0.22, e = 0.0, f = 1.6 }
    , probability = 7.0e-2
    }
  , { transformation = { a = -0.15, b = 0.28, c = 0.26, d = 0.24, e = 0.0, f = 0.44 }
    , probability = 7.0e-2
    }
  ]
}

\end{lstlisting}

[fn:floating_point] Unfortunately, Dhall explicitly does not allow floating-point arithmetic.
As such, one still needs to write e.g. $1/3$ as $0.3333333333333333$.

*** Rendering
The program computes the point cloud once, on startup, and then re-renders the image that is shown in the program window
every time the user moves the camera.

Rendering is done by iterating (in parallel) over all points in the point cloud and filling a two-dimensional histogram with the same dimensions
as the canvas with numbers. 
This histogram is then used to draw the attractor (any non-empty pixel is colored white and the rest black).

*** Moving the camera
The camera can be moved by either zooming in or out using the scrollwheel,
or translating the camera by dragging with the mouse.

These operations alter the camera's current view transformation, 
which is stored as a transformation matrix relative to unit space.

*** Performing 'self-similarity jumping'

While the program is running, a user can go back to a more shallow view by pressing `+`[fn:plus], and then when inside one or multiple shallower views,
`-` can be pressed to undo the last jump.

Care is taken to only allow the jump up if the current camera viewport is fully contained within one mapping's region.

This process was intentionally kept manual, because it allows the user to more easily compare how
the visualization looks with and without the jumping, and allows for a full exploration of the circumstances
in which a jump up is and is not actually correct (see autoref:subsection:jumping_restrictions).

[fn:plus] Strictly speaking, by pressing the `=' key; pressing SHIFT is not necessary.
*** Rendering `guides'

To make it easier to see how an IFS is constructed, as well as easier for a user to orient themselves when
testing the 'self-similarity jumping', it is possible to toggle the rendering of `guides' by pressing the `g' key.
Similarly, the rendering of points can be toggled by pressing the `p' key (allowing one to see the guides more clearly, when desired).

These 'guides' are the unit squre, after undergoing a sequence of zero, one, two etc. mappings of the IFS.
Different colors are used for guides at different sequence-depths.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_points}
         \caption{only points}
         \label{figure:barnsley_guides}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_guides_and_points}
         \caption{guides and points}
         \label{figure:barnsley_guides_and_points}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/barnsley_guides}
         \caption{only guides}
         \label{figure:barnsley_guides}
     \end{subfigure}
        \caption{The Barnsley Fern (\autoref{ifs:barnsley_fern}) rendered by the program in different ways.}
        \label{figure:barnsley_guides_vs_points}
\end{figure}

[fn:EDSL] Embedded Domain-Specific Language.
[fn:debugging] Instead of being presented with a black screen when a programming mistake is made, 
Accelerate presents errors at compile-time in many cases. Furthermore, Accelerate features a
single-threaded reference implementation that runs on the CPU that can be used to sanity-check the behaviour of code.

* Findings
\label{section:findings}

** Restrictions on `self-similarity jumping'
\label{subsection:jumping_restrictions}

From experimentation with the program it turns out that there are two common situations
in which the technique outlined in autoref:subsection:self_similarity cannot be used.

**** Borders between transformations

It is rather common to zoom in on the borders between transformations, as this is often
where interesting visual details of the IFS might appear.

However, autoref:algorithm:self_similarity_jump_up is not able to handle borders between transformations,
thus making it useless in these scenarios.

An example can be seen in figure autoref:figure:sierpinsky_transformation_borders.

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{figures/sierpinsky_transformation_borders}
\caption{In this example the camera viewport (indicated in cyan) overlaps both $f_1$ and $f_2$ of \autoref{ifs:sierpinsky} partially. This case is not handled by \autoref{algorithm:self_similarity_jump_up}. }
\label{figure:sierpinsky_transformation_borders}
\end{figure}

**** Overlapping subtransformations

\begin{figure}
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{figures/barnsley_colored_jump1}
         \caption{Since the camera viewport only sees points of $f_2$, the jump up is proper.}
         \label{figure:barnsley_jump_a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{figures/barnsley_colored_jump2}
         \caption{Since the camera viewport sees both points of $f_2$ and $f_3$, the jump is incorrect. Note the leaf in the lower left missing after the jump.}
         \label{figure:barnsley_jump_b}
     \end{subfigure}
        \caption{Problems when jumping up on \autoref{ifs:barnsley_fern}. The top row of each figure is `world space' with the camera viewport indicated as white dashed polygon. 
The bottom row shows the camera viewport.
Points colored based on their latest mapping. }
        \label{figure:barnsley_jump}
\end{figure}



A more shallow view of the attractor only shows the same as a deeper view when
there are no points transformed by another mapping that end up in the deeper view.

When there are points from another mapping in the current view, 
going to a more shallow view will make points disappear from the perspective of the user.
In practice, this means that for many IFSs there are large regions in which the technique cannot be used at all.

Simple IFSs like the Sierpińsky Triangle (autoref:ifs:sierpinsky) in which transformations
do not overlap, do not exhibit this problem. 
Slightly more complex IFSs like the Dragon Curve (autoref:ifs:dragon_curve) or the Barnsley Fern (autoref:ifs:barnsley_fern) however do.
See autoref:figure:dragon_curve_overlaps for an graphical explanation.


This case is annoyingly common and there is no clear solution to alleviate this restriction.
Furthermore, it is not simple to check whether we are currently in a region that exhibits the problem,
as this would require evaluating the IFS itself.

It is possible to take a rough `upper bound' estimate of the disallowed regions by keeping track, 
per mapping, where the unit square would end up after a couple of mappings with this mapping as last (i.e. most significant) one.


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_a}
         \caption{The unit square (indicated in white) transformed by $f_1$ and $f_2$ (indicated in green)}
         \label{figure:dragon_curve_a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_b}
         \caption{Subtransformations of $f_1$: $f_1 \circ f_1$ and $f_1 \circ f_2$ (indicated in red)}
         \label{figure:dragon_curve_b}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_c}
         \caption{Subtransformations of $f_2$: $f_2 \circ f_1$ and $f_2 \circ f_2$. (indicated in blue)}
         \label{figure:dragon_curve_c}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dragon_curve_d}
         \caption{The region in which (b) and (c) overlap (indicated in magenta)}
         \label{figure:dragon_curve_d}
     \end{subfigure}
        \caption{Showing the first couple of iterations of rendering the attractor of the dragon curve IFS $D$ (\autoref{ifs:dragon_curve}), and the regions in which (sequences of) transformations overlap.}
        \label{figure:dragon_curve_overlaps}
\end{figure}


** Memory Usage

Point clouds take up a lot of memory on the GPU. To render a fractal at reasonable detail, depending on the particular IFS,
hundreds of millions if not billions of points are necessary.

A reasonable way to store a point cloud is by using 32 bits for each of the two coordinates of a point. 
This means that one point takes up exactly one machine word of a 64-bit computer system.
Stored this way, a point cloud of 100,000,000 points requires 0.596 GiB of GPU memory,
and 1,000,000,000 points requires 5.96 GiB.
For current generation GPUs[fn:GPU2020], this often is more memory than available.

[fn:GPU2020] At the time of writing, high-end consumer GPUs contain somewhere between 2 and 24 GiB of available memory. cite:ign2020topgpus

* Conclusion
\label{section:conclusion}

A program was implemented which has shown that there is /some/ merit to rendering an IFS's attractor using a point cloud as re-usable intermediate structure.
However, the self-similarity detection method that was proposed turns out to be unusable in common cases.

Therefore, the proposed technique can be considered of limited practicality, 
at least until a more sophisticated self-similarity detection method is found.

* Future Work
\label{section:further_work}

It is our hope that a more sophisticated way of detecting self-similarity might be found,
which would make `self-similarity jumping' more practical.

Besides this, while we have shown in a proof-of-concept program that it is possible to render an IFS using a point cloud
with a reasonable speed, there are many optimizations that could be made to make the program run faster (potentially even in real-time),
most notably the rendering optimizations listed in autoref:subsection:point_cloud_optimizations

Another venue that could be explored is the rendering of an IFS's attractor at multiple `levels of detail':
It might be possible to create more detailed local versions of the point cloud (based on the points of the less detailed point cloud) when the user
zooms in on a particular region, on demand.

Finally it is worth noting that cite:lawlor2012gpu already presents an efficient way to render 
a large set of IFSs using a very different approach (c.f. autoref:subsection:deterministic_gpu), 
which might be worthwhile to be explored further.

\pagebreak
\printbibliography
\clearpage

\appendix

* Iterated Function Systems used

This appendix lists the mapping functions of the IFSs that were used throughout this thesis.

\begin{ifs}
\centering
\includegraphics[width=0.2\textwidth]{figures/sierpinsky}
$$ 
\begin{aligned}
f_1(x, y) &= \begin{bmatrix} \frac{1}{2} & 0 \\ 0 & \frac{1}{2} \end{bmatrix} & \begin{bmatrix} x \\ y \end{bmatrix} & , &p_1 = \frac{1}{3} \\
f_2(x, y) &= \begin{bmatrix} \frac{1}{2} & 0 \\ 0 & \frac{1}{2} \end{bmatrix} & \begin{bmatrix} x \\ y \end{bmatrix} &+ \begin{bmatrix} \frac{1}{2} \\ 0 \end{bmatrix} , &p_2 = \frac{1}{3} \\
f_3(x, y) &= \begin{bmatrix} \frac{1}{2} & 0 \\ 0 & \frac{1}{2} \end{bmatrix} & \begin{bmatrix} x \\ y \end{bmatrix} &+ \begin{bmatrix} \frac{1}{4} \\ \frac{\sqrt{3}}{4} \end{bmatrix} , &p_3 = \frac{1}{3} \\
\end{aligned}
$$

\caption{the Sierpi\'nsky triangle}
\label{ifs:sierpinsky}
\end{ifs}

\begin{ifs}
\centering
\includegraphics[width=0.2\textwidth]{figures/dragon_curve}
$$ 
\begin{aligned}
f_1(x,y) &= \frac{1}{\sqrt{2}}\begin{bmatrix} \cos 45^\circ & -\sin 45^\circ \\ \sin 45^\circ & \cos 45^\circ \end{bmatrix} & \begin{bmatrix} x \\ y \end{bmatrix} & , &p_1 = \frac{1}{2} \\
f_2(x,y) &= \frac{1}{\sqrt{2}}\begin{bmatrix} \cos 135^\circ & -\sin 135^\circ \\ \sin 135^\circ & \cos 135^\circ \end{bmatrix} & \begin{bmatrix} x \\ y \end{bmatrix} &+ \begin{bmatrix} 1 \\ 0 \end{bmatrix} , &p_2 = \frac{1}{2} \\
\end{aligned}
$$

\caption{the Heighway Dragon Curve}
\label{ifs:dragon_curve}
\end{ifs}

\begin{ifs}
\centering
\includegraphics[width=0.2\textwidth]{figures/barnsley_100000000}
$$ 
\begin{aligned}
f_1(x,y) &= \begin{bmatrix} \ 0.00 & \ 0.00 \ \\ 0.00 & \ 0.16 \end{bmatrix}  & \begin{bmatrix} \ x \\ y \end{bmatrix} & , &p_1 = 0.01 \\
f_2(x,y) &= \begin{bmatrix} \ 0.85 & \ 0.04 \ \\ -0.04 & \ 0.85 \end{bmatrix} & \begin{bmatrix} \ x \\ y \end{bmatrix} &+ \begin{bmatrix} \ 0.00 \\ 1.60 \end{bmatrix} , &p_2 = 0.85 \\
f_3(x,y) &= \begin{bmatrix} \ 0.20 & \ -0.26 \ \\ 0.23 & \ 0.22 \end{bmatrix} & \begin{bmatrix} \ x \\ y \end{bmatrix} &+ \begin{bmatrix} \ 0.00 \\ 1.60 \end{bmatrix} , &p_3 = 0.07 \\
f_4(x,y) &= \begin{bmatrix} \ -0.15 & \ 0.28 \ \\ 0.26 & \ 0.24 \end{bmatrix} & \begin{bmatrix} \ x \\ y \end{bmatrix} &+ \begin{bmatrix} \ 0.00 \\ 0.44 \end{bmatrix} , &p_4 = 0.07 \\
v(x, y) &= \begin{bmatrix} 0.09 & \ 0.00 \\ 0.00 & \ -0.09 \end{bmatrix} & \begin{bmatrix} \ x \\ y \end{bmatrix} &+ \begin{bmatrix} \ 0.50 \\ 1.00 \end{bmatrix} \\
\end{aligned}
$$
\caption{the Barnsley Fern}
\label{ifs:barnsley_fern}
\end{ifs}

