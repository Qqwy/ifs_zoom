#+TITLE: \Huge In by Out again
#+SUBTITLE: Faking arbitrarily-deep zooming on Iterated Function Systems

#+BIND: org-latex-prefer-user-labels t

#+LATEX_HEADER: \setlength{\parindent}{1em}
#+LATEX_HEADER: \setlength{\parskip}{0.5em}
#+LATEX_HEADER: \usepackage[citestyle=alphabetic,bibstyle=alphabetic, hyperref=true, backref=true,maxcitenames=3,url=true,backend=biber,natbib=true] {biblatex}
#+LATEX_HEADER: \addbibresource{bibliography.bib}

#+LATEX_HEADER: \usepackage[a4paper, total={7in, 9in}]{geometry}

#+LATEX_HEADER: \usepackage[ruled, procnumbered]{algorithm2e}

# not emph
#+LATEX_HEADER: \SetArgSty{}

#+LATEX_HEADER: \usepackage[dvipsnames]{xcolor}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{pifont}
#+LATEX_HEADER: \newcommand{\cmark}{\color{ForestGreen}\ding{52}}%
#+LATEX_HEADER: \newcommand{\xmark}{\color{Maroon}\ding{55}}%

#+LATEX_HEADER: \hypersetup{colorlinks=true}

#+LATEX_HEADER: \renewcommand{\sectionautorefname}{{\color{Black}\S}}
#+LATEX_HEADER: \renewcommand{\subsectionautorefname}{{\color{Black}\S\S}}
#+LATEX_HEADER: \renewcommand{\subsubsectionautorefname}{{\color{Black}\S\S\S}}
#+LATEX_HEADER: \renewcommand{\functionautorefname}{{\color{Black}\textbf{Function}}\color{Magenta}}
#+LATEX_HEADER: \renewcommand{\algorithmautorefname}{{\color{Black}\textbf{Algorithm}}\color{Magenta}}


#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage[shortlabels]{enumitem}

#+LATEX_HEADER: \usepackage{newfloat}
#+LATEX_HEADER: \DeclareFloatingEnvironment[fileext=lol, listname={List of L-system definitions}, name=L-system, placement=tbhp, within=section]{lsystem}

#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{todonotes}

#+LATEX_HEADER: \usepackage{pifont,kantlipsum}
#+LATEX_HEADER: \newcommand*{\altasterism}{\vspace*{1em plus .5em minus .5em}\noindent\hspace*{\fill}\ding{104}\hspace*{\fill}}



#+LATEX_HEADER: \usepackage{rugscriptie}
#+LATEX_HEADER: \supervisor{dr. J. Kosinka}
#+LATEX_HEADER: \supervisor{G. J. Hettinga}
#+LATEX_HEADER: \date{August 2020}
#+LATEX_HEADER: \faculty{fwn} % Or feb, fgg, fgmw, fl, frg, frw, fw, umcg
#+LATEX_HEADER: \thesistype{Bachelors's thesis} % Will be printed unmodified

#+OPTIONS: toc:nil
\pagebreak
#+TOC: headlines 3

* Abstract

* Introduction

Iterated Function Systems (IFSs) are a method to generate infinitely detailed fractal images 
by repeatedly applying simple mathematical functions until a fixed point is reached. [CITE]
IFSs see use in rendering/modeling of physical phenomena[CITE], image compression [CITE] and representing gene structures [CITE].
Sometimes they also see use simply for the aesthetic beauty of their graphical representations[CITE].

Various computer algorithms to visualize IFSs exist [CITE], [CITE].
However, these all take either a still image as final result, or, if they want to render an animation,
view this as a sequence of separate still images to generate.

This leaves a door open for potential optimization: if there is information that remains the same between animation frames, 
then we could compute it only once and re-use this information for all frames.

For instance, many kinds of animations consist of transformations of the camera viewport w.r.t the viewed fractal over time like translation, rotation and scaling do not require alterations to the fractal itself.
This means that (an approximation of) the fractal might be computed once and then be used for all frames.

Furthermore, because of the self-similar nature of the rendered fractals,
it might it be possible to simulate zooming in to an arbitrary depth by 'jumping up' to a more shallow viewport
that shares the same self-similarity as the original one. 

Investigating this claim in detail is the essence of this thesis.

** Overview


* Background
\label{section:background}
  
Informally, an Iterated Function System is a set of transformations that, given any input image, can create a new image by

1. transforming the input image with each of the transformations
2. combining all transformed images together. This is the new image.

This process is then repeated an arbitrary number of times, until changes between the input image and output image are no longer visible to the human eye.

What you end up with is a visual representation of the IFS's attractor.

\todo[inline]{reference picture}


** Formal definition of an Iterated Function System

Formally, an Iterated Function System consists of a finite set of contraction mappings that map a complete metric space $(\mathcal{M}, d)$ to itself:

$$ = \{ f_i : \mathcal{M} \rightarrow \mathcal{M} | i = 1, 2, \ldots, N \}, N \in \mathbb{N}$$

That each mapping needs to be contractive means that for each mapping $f_i$, the distance between every two arbitrary points $a$ and $b$ in $(\mathcal{M}, d)$ needs to be larger than the distance of the points after transforming them:

$$d(f_i(a), f_i(b)) < d(a, b)$$

We can then take the union of performing all of these mappings on any compact set of points $\mathcal{S}_0 \subset \mathcal{M}$. This procedure is called the /Hutchkinson Operator/ ($H$). 
We can iterate it as often as we'd like:

$$ \mathcal{S}_{n + 1} = H(\mathcal{S}_n) = \bigcup_{i=1}^{N} f_i(\mathcal{S}_n) $$

If we perform this operation an arbitrary number of times, we approach the fixed-point or attractor, $\mathcal{A}$, of the Iterated Function System:

$$\mathcal{A} = \lim_{n \rightarrow \infty} \mathcal{S}_n$$

Curiously, which set of points $\mathcal{S}_0$ we started with makes no difference (we might even start with a single point) [CITE].

\altasterism

Most research of IFSs restricts itself to using $\mathbb{R}^2$ as metric space[fn:euclidean] which can easily be rendered to screen or paper,
and furthermore most commonly-used IFSs are restricted to use /affine transformations/ as mappings.

Because of their prevalence, these are also the restrictions that will be used in this thesis.

[fn:euclidean] More formally, the two-dimensional Euclidean space: $\left(\mathbb{R}^2, d(p, q) = \sqrt{p - q)^2}\right)$.

** Rendering an Iterated Function System

A couple of algoritms exist to render (visualize) the attarctor of an Iterated Function System. 
While it is impossible to render the attractor exactly, as this would require an infinite number of transformation steps,
we can approximate it until we are certain that the difference between our approximation and the attractor is smaller than
the smallest thing we can visually represent (e.g. smaller than the size of a pixel).

Because we apply $H$ many times and each time consists of taking the union of $N$ different transformations,
the result can be seen as traversing an (infinitely deep) tree of transformations, 
where each sub-tree is self-similar to the tree as a whole.

Different algorithms take different approaches to evaluating this tree (up to a chosen finite depth).

More in-depth information about the rendering of Iterated Function Systems can be found [CITE]. 
Short summaries of the two most common techniques will now follow.

# All of the rendering techniques have in common that the iteration of the Hutchkinson operator is seen as an (infinitely deep) tree with branching factor $N$.
# We traverse this tree up to a certain depth.

\todo[inline]{PICTURE OF THIS TREE}


*** The deterministic method

In this approach we evaluate the whole tree up to a chosen depth. The algorithm works as follows:

1. Pick a starting point $z_0$;
2. Traverse the tree down to the chosen depth $k$, building up a sequence of transformations [fn:function_composition]
   $f_{i_k} \circ \ldots \circ f_{i_1}$;
3. For each node at this depth, evaluate and render $z_k = (f_{i_k} \circ \ldots \circ f_{i_1})(z_0) = f_{i_k-1}(z_{k-1})$;

Since $z_{k} = f_{i_k-1}(z_{k-1})$ this procedure takes, for an approximation that consists of $N$ points, depending on the tree traversal chosen:

- a linear amount ( $\mathcal{O}(N)$ ) of memory  for a breadth-first tree-traversal.
- a logarithmic amount ( $\mathcal{O}(\log{N})$ ) of memory for a depth-first tree-traversal.

The advantage of the breadth-first traversal is that generation could be stopped interactively,
while the depth-first traversal requires the stopping criterion to be known beforehand. [CITE]

While the deterministic method is easy to understand (and indeed is a direct translation of the informal process described at the start of autoref:section:background),
it is usually less efficient and more complex to implement on a computer than the algorithm that will be described next.

[fn:function_composition] $\circ$ stands for function composition: $(f \circ g)(x) = f(g(x))$. 
Be aware that when affine transformation functions are represented as matrices (e.g. $F$ and $G$), matrix multiplication is in the opposite order ($f \circ g \equiv G \cdot F$)

*** The chaos game

The /stochastic method/[CITE], also known as the /random iteration algorithm/[CITE] or more frequently the /chaos game/, works as seen in autoref:chaosGame

\begin{algorithm}[H]
\caption{the chaos game}
\label{chaosGame}
  $n$: the number of transformations the IFS consists of. \\
  $z$: a random point on the screen  \\
  \While{less than $N$ points plotted}{ 
    $i$: a random integer between $0$ and $n$.  \\
    $z = f_i(z)$  \\
    render($z$) except during the first $x$ iterations \\
  }

\end{algorithm}

This method converges to a correct result because of the following two facts:

- because the precision of the canvas we render on is finite, and because all transformations are contracting,
 two points $a$ and $b$ are indistinguishable after only $x$ transformations.
  In other words, only the latest $x$ transformations determine at what location on the canvas a point will end up (with the latest transformation having the largest effect on the point's final location).[fn:contraction]
- at each depth in the tree the subtree remains the same, so every sequence of transformations approaches the attractor.

Therefore, all intermediate points after the first $x$ iterations are visually indistinguishable from the a point that is part of the attractor.
By running this non-deterministic approach for enough iterations we approach a diverse enough set of 'transformation sequences of length $x$' that we end up covering the whole attractor.

The nice thing about the chaos game is that it does not require any extra memory (besides the point $z$).
Also, because it is so simple and little auxiliary memory is needed, it runs very efficiently on modern CPU architectures.

A disadvantage of the chaos game is that the result is by its very nature /non-deterministic/.
If not enough points are used, the result might end up 'grainy' and it is not predictable what part of the attractor will be covered.

One further disadvantage the chaos game has, is that in its simplest form, all transformations have an equally likely chance to be used.
However, because some transformations might be (much) more contracting than others, this means that coverage of the attractor is not even,
which means that we need to use much more iterations than would be the case if we balance it out.

Therefore, most implementations of the chaos game allow (or require) the user to specify a /probability/ for each transformation.
All these probabilities together ought to sum up to 1.[fn:probabilities]

\altasterism

Because of its simplicity and efficiency, the chaos game is used more frequently than the deterministic method for practical implementations.
The chaos game is also easier to paralellize for Graphical Processor Units (GPUs), as will be outlined in the next subsection.

[fn:contraction] Methods for precisely determining the lower and upper bounds of IFS contraction for a particular IFS (and therefore the exact value of $x$) exist [CITE], 
but are not relevant for this thesis.

[fn:probabilities] These probabilities are often fine-tuned by hand, although algorithms to determine balanced probabilities exist as well [CITE] section 2.4.

** IFS rendering on a GPU

It is enticing to port IFS rendering to run on GPU-architecture because to produce a smooth image, often hundreds of millions of points are needed.

However, optimizing IFS rendering to run well on GPU-architectures is a bit of a challenge.

GPU shaders usually operate by running a check for every pixel on the final canvas, to determine its color.
For other fractals like the Mandelbrot- and Julia-sets, this is a natural fit since the construction of those fractals works exactly in that way.

For an IFS this does not work, as an IFS is created in the other direction. Points end up at some location on the canvas /only after transforming/ many times.
Attempts to go the other way fall flat, for instance because this would require to invert the IFS' mappings, but they are not guaranteed to be invertible.

Instead, General-Purpose GPU-techniques are used that are able to use the top-down approach.

*** the chaos game on the GPU

The deterministic method is difficult to paralellize on the GPU because of the extra memory that is required to keep track of the current position in the tree.
Coordinating which GPU thread would calculate which part of the tree and sharing results would be a hassle.

Instead, the chaos game is more frequently used because of its simplicity. It is paralellized in a straightforward way, by running the iteration process many times side-by-side (one per GPU thread),
and then combine the final results of all of these on a single canvas. [CITE]

*** the deterministic method on the GPU

An exciting approach taken in [CITE] uses the deterministic method instead:
by using the fast inverse square root operation, even unbounded (noncontracting) and nonlinear IFSs can be efficiently
evaluated using the deterministic method, programmed in normal GPU shaders that manipulate a couple of GPU textures.




* Problem Description

In the last section, the construction of an IFS's attractor was formally defined, 
and different approaches of rendering it were outlined.

While many different approaches to IFS rendering exist, some of them quite efficient,
none of them re-uses information from rendering one image of the IFS for rendering another.

This leads us to the research question of this thesis:

**Is it possible to render animations in which a camera zooms in on an Iterated Function System's attractor in real-time?**

* Approach

** Design
# Focus on simplicity

*** Point Cloud

** Implementation




* Findings

# Zooming out is possible iff
# - The camera is fully contained inside the transformation
# - The camera is not overlapping any of the other transformations' 'immediate child-transforms'

# Further restriction: when viewport overlaps multiple transformations there is a problem, 
# even if this would theoretically be a possibility. Maybe there are smarter ways of recognizing self-similarity?


* Discussion

# Point clouds are big, and you need many points for an IFS. Keeping them in memory for a GPU is a hassle.

# Maybe comment on Accelerate's lack of certain functions?

* Conclusion

* Further Work

# List of things that could be improved on in the implementation
# - optimizing rendering of the point cloud
# - optimizing creation of the point cloud
# - investigate different ideas of recognizing self-similarity
